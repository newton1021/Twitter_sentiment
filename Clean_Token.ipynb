{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/theDoctor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='latin-1')\n",
    "    df.columns = [\"text\",\"sentiment\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_text_data(filename, append = False):\n",
    "    if not append:\n",
    "        data = []\n",
    "        \n",
    "    textfile = open(filename, \"r\")\n",
    "    for tweet in textfile:\n",
    "        parts = tweet.split(',')\n",
    "        if len(parts) > 2:\n",
    "            newTweet = [parts[0],parts[1], 0, \"\".join(parts[3:])]\n",
    "            data.append(newTweet)\n",
    "    textfile.close()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = [\"User\", \"Date\", \"Retweet\", \"text\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def delete_redundant_cols(df, cols):\n",
    "    for col in cols:\n",
    "        del df[col]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi prepar exam'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\",\"\",tweet, flags=re.MULTILINE)   \n",
    "\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "    tweet = re.sub(r'\\@\\w+|\\#',\"\",tweet)\n",
    "\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stop_words]\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "\n",
    "    return \" \".join(lemma_words)\n",
    "preprocess_tweet_text(\"Hi there, how are you preparing for your exams?\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_string(sentiment):\n",
    "    if sentiment < -0.1:\n",
    "        return \"Negative\"\n",
    "    elif sentiment < 0.1:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_text_data(\"datasets/dataset 2021090613_29_33.txt\")\n",
    "# Remove unwanted columns from dataset\n",
    "# n_dataset = remove_unwanted_cols(dataset, ['t_id', 'created_at', 'query', 'user'])\n",
    "#Preprocess data\n",
    "dataset[\"cleanText\"] = dataset['text'].apply(preprocess_tweet_text)\n",
    "\n",
    "\n",
    "\n",
    "# determine the sentiment of the tweet using TextBlob and use that result to train the model\n",
    "sent = []\n",
    "for x in dataset['cleanText']:\n",
    "    s = TextBlob(x).sentiment.polarity\n",
    "    if s < -0.1:\n",
    "        r = -1\n",
    "    elif s > 0.1:\n",
    "        r = 1\n",
    "    else:\n",
    "        r = 0\n",
    "    sent.append(r) \n",
    "#create the sentiment column\n",
    "dataset[\"sentiment\"] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727273\n",
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into Train, Test\n",
    "\n",
    "# Same tf vector will be used for Testing sentiments on unseen trending data\n",
    "tf_vector = get_feature_vector(np.array(dataset['cleanText']).ravel())\n",
    "X = tf_vector.transform(np.array(dataset['cleanText']).ravel())\n",
    "y = np.array(dataset['sentiment']).ravel()\n",
    "# y = y.astype('float')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "# Training Naive Bayes model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_predict_nb = NB_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_nb))\n",
    "\n",
    "# Training Logistics Regression model\n",
    "LR_model = LogisticRegression(solver='lbfgs')\n",
    "LR_model.fit(X_train, y_train)\n",
    "y_predict_lr = LR_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   User       52 non-null     object\n",
      " 1   Date       52 non-null     object\n",
      " 2   Retweet    52 non-null     int64 \n",
      " 3   text       52 non-null     object\n",
      " 4   cleanText  52 non-null     object\n",
      " 5   sentiment  52 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 2.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two001snake</td>\n",
       "      <td>Mon Sep 06 18:26:37 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>ALL #Corrupt @USCongress #FakeNews #MSM #TERR...</td>\n",
       "      <td>corrupt uscongress fakenew msm terrifi wethepe...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shepersists2</td>\n",
       "      <td>Mon Sep 06 18:23:54 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>I am anti war but the way Americans left Afgh...</td>\n",
       "      <td>anti war way american left afghanistan disgust...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>APissedOffCons1</td>\n",
       "      <td>Mon Sep 06 17:59:17 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>If you were trying to destroy the country wou...</td>\n",
       "      <td>tri destroy countri would anyth differ biden now…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               User                             Date  Retweet  \\\n",
       "4       two001snake   Mon Sep 06 18:26:37 +0000 2021        0   \n",
       "9      shepersists2   Mon Sep 06 18:23:54 +0000 2021        0   \n",
       "47  APissedOffCons1   Mon Sep 06 17:59:17 +0000 2021        0   \n",
       "\n",
       "                                                 text  \\\n",
       "4    ALL #Corrupt @USCongress #FakeNews #MSM #TERR...   \n",
       "9    I am anti war but the way Americans left Afgh...   \n",
       "47   If you were trying to destroy the country wou...   \n",
       "\n",
       "                                            cleanText  sentiment  \n",
       "4   corrupt uscongress fakenew msm terrifi wethepe...         -1  \n",
       "9   anti war way american left afghanistan disgust...         -1  \n",
       "47  tri destroy countri would anyth differ biden now…         -1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['sentiment'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>global_police</td>\n",
       "      <td>Mon Sep 06 18:29:09 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@TheEconomist Good it's such a wonderful time...</td>\n",
       "      <td>theeconomist good wonder time sister biden</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>twistedcomputer</td>\n",
       "      <td>Mon Sep 06 18:24:35 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#Maddow gets paid millions as #Biden's modern...</td>\n",
       "      <td>maddow get paid million biden modern day tokyo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SaintlySicilian</td>\n",
       "      <td>Mon Sep 06 18:20:32 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>I know that in certain countries (North Korea...</td>\n",
       "      <td>know certain countri north korea former soviet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00001Kat</td>\n",
       "      <td>Mon Sep 06 18:18:44 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Wait-I thought we were going to “follow the #...</td>\n",
       "      <td>waiti thought go “ follow scienc ” much biden ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>clayandbuck</td>\n",
       "      <td>Mon Sep 06 18:14:27 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What does the #Biden administration have to ...</td>\n",
       "      <td>biden administr hang hat right claytravi biden...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>DickMorrisTweet</td>\n",
       "      <td>Mon Sep 06 18:13:31 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>People Think Better Of Trump As They Get To K...</td>\n",
       "      <td>peopl think good trump get know biden – lunch ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ChuckNorton1</td>\n",
       "      <td>Mon Sep 06 18:12:24 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Taliban Holding 6 planes full of Americans an...</td>\n",
       "      <td>taliban hold 6 plane full american siv hostag</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>leylaboulton</td>\n",
       "      <td>Mon Sep 06 18:04:40 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Let’s hope the #Biden administration can do m...</td>\n",
       "      <td>let ’ hope biden administr protect women ’ rig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               User                             Date  Retweet  \\\n",
       "1     global_police   Mon Sep 06 18:29:09 +0000 2021        0   \n",
       "6   twistedcomputer   Mon Sep 06 18:24:35 +0000 2021        0   \n",
       "18  SaintlySicilian   Mon Sep 06 18:20:32 +0000 2021        0   \n",
       "21         00001Kat   Mon Sep 06 18:18:44 +0000 2021        0   \n",
       "29      clayandbuck   Mon Sep 06 18:14:27 +0000 2021        0   \n",
       "30  DickMorrisTweet   Mon Sep 06 18:13:31 +0000 2021        0   \n",
       "33     ChuckNorton1   Mon Sep 06 18:12:24 +0000 2021        0   \n",
       "39     leylaboulton   Mon Sep 06 18:04:40 +0000 2021        0   \n",
       "\n",
       "                                                 text  \\\n",
       "1    @TheEconomist Good it's such a wonderful time...   \n",
       "6    #Maddow gets paid millions as #Biden's modern...   \n",
       "18   I know that in certain countries (North Korea...   \n",
       "21   Wait-I thought we were going to “follow the #...   \n",
       "29   \"What does the #Biden administration have to ...   \n",
       "30   People Think Better Of Trump As They Get To K...   \n",
       "33   Taliban Holding 6 planes full of Americans an...   \n",
       "39   Let’s hope the #Biden administration can do m...   \n",
       "\n",
       "                                            cleanText  sentiment  \n",
       "1          theeconomist good wonder time sister biden          1  \n",
       "6   maddow get paid million biden modern day tokyo...          1  \n",
       "18  know certain countri north korea former soviet...          1  \n",
       "21  waiti thought go “ follow scienc ” much biden ...          1  \n",
       "29  biden administr hang hat right claytravi biden...          1  \n",
       "30  peopl think good trump get know biden – lunch ...          1  \n",
       "33      taliban hold 6 plane full american siv hostag          1  \n",
       "39  let ’ hope biden administr protect women ’ rig...          1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['sentiment'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file_name = \"trending_tweets/08-04-2020-1586291553-tweets.csv\"\n",
    "# test_ds = load_dataset(test_file_name, [\"t_id\", \"hashtag\", \"created_at\", \"user\", \"text\"])\n",
    "# test_ds = remove_unwanted_cols(test_ds, [\"t_id\", \"created_at\", \"user\"])\n",
    "\n",
    "# # Creating text feature\n",
    "# test_ds.text = test_ds[\"text\"].apply(preprocess_tweet_text)\n",
    "# test_feature = tf_vector.transform(np.array(test_ds.iloc[:, 1]).ravel())\n",
    "\n",
    "# # Using Logistic Regression model for prediction\n",
    "# test_prediction_lr = LR_model.predict(test_feature)\n",
    "\n",
    "# # Averaging out the hashtags result\n",
    "# test_result_ds = pd.DataFrame({'hashtag': test_ds.hashtag, 'prediction':test_prediction_lr})\n",
    "# test_result = test_result_ds.groupby(['hashtag']).max().reset_index()\n",
    "# test_result.columns = ['heashtag', 'predictions']\n",
    "# test_result.predictions = test_result['predictions'].apply(int_to_string)\n",
    "\n",
    "# print(test_result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "c1e83760cadaf87b0a0f9ca95f844ef4947bb6840a3db1438c78c07aa1fb0afe"
  },
  "kernelspec": {
   "display_name": "Python Data",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
