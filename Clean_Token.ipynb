{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/theDoctor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='latin-1')\n",
    "    df.columns = [\"text\",\"sentiment\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_text_data(filename, append = False):\n",
    "    if not append:\n",
    "        data = []\n",
    "        \n",
    "    textfile = open(filename, \"r\")\n",
    "    for tweet in textfile:\n",
    "        parts = tweet.split(',')\n",
    "        if len(parts) > 2:\n",
    "            newTweet = [parts[0],parts[1], 0, \"\".join(parts[3:])]\n",
    "            data.append(newTweet)\n",
    "    textfile.close()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = [\"User\", \"Date\", \"Retweet\", \"text\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def delete_redundant_cols(df, cols):\n",
    "    for col in cols:\n",
    "        del df[col]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi preparing exams'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\",\"\",tweet, flags=re.MULTILINE)   \n",
    "\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "    tweet = re.sub(r'\\@\\w+|\\#',\"\",tweet)\n",
    "\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stop_words]\n",
    "\n",
    "#     ps = PorterStemmer()\n",
    "#     stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]\n",
    "\n",
    "    return \" \".join(lemma_words)\n",
    "preprocess_tweet_text(\"Hi there, how are you preparing for your exams?\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_string(sentiment):\n",
    "    if sentiment < -0.1:\n",
    "        return \"Negative\"\n",
    "    elif sentiment < 0.1:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(classification='neg', p_pos=0.061848992071306515, p_neg=0.9381510079286948)\n",
      "Sentiment(classification='pos', p_pos=0.8324124318829695, p_neg=0.1675875681170304)\n",
      "Sentiment(classification='neg', p_pos=0.3947368421052628, p_neg=0.6052631578947371)\n",
      "Sentiment(classification='pos', p_pos=0.6571729015199701, p_neg=0.34282709848003085)\n",
      "Sentiment(classification='pos', p_pos=0.697496358192723, p_neg=0.30250364180727796)\n",
      "Sentiment(classification='pos', p_pos=0.84646178324069, p_neg=0.1535382167593091)\n",
      "Sentiment(classification='neg', p_pos=0.46076328060400257, p_neg=0.5392367193959956)\n",
      "Sentiment(classification='pos', p_pos=0.5387783635019395, p_neg=0.4612216364980616)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.8832313000416809, p_neg=0.11676869995831875)\n",
      "Sentiment(classification='pos', p_pos=0.5953133228014746, p_neg=0.4046866771985255)\n",
      "Sentiment(classification='pos', p_pos=0.7366632825412875, p_neg=0.26333671745871584)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.9772751778514385, p_neg=0.02272482214855901)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.7503410007886737, p_neg=0.24965899921132556)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.9854599805988188, p_neg=0.014540019401178854)\n",
      "Sentiment(classification='pos', p_pos=0.5168871501131187, p_neg=0.48311284988688197)\n",
      "Sentiment(classification='pos', p_pos=0.8478153195563332, p_neg=0.1521846804436644)\n",
      "Sentiment(classification='pos', p_pos=0.5430586948045787, p_neg=0.4569413051954218)\n",
      "Sentiment(classification='neg', p_pos=0.4285243383415213, p_neg=0.5714756616584777)\n",
      "Sentiment(classification='pos', p_pos=0.5043372294908613, p_neg=0.49566277050913865)\n",
      "Sentiment(classification='neg', p_pos=0.37500000000000044, p_neg=0.6249999999999998)\n",
      "Sentiment(classification='pos', p_pos=0.75, p_neg=0.2499999999999997)\n",
      "Sentiment(classification='pos', p_pos=0.7539372683057236, p_neg=0.24606273169427836)\n",
      "Sentiment(classification='pos', p_pos=0.8281640040026621, p_neg=0.1718359959973357)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='neg', p_pos=0.1803212361203609, p_neg=0.8196787638796381)\n",
      "Sentiment(classification='neg', p_pos=0.11503524477202483, p_neg=0.8849647552279747)\n",
      "Sentiment(classification='neg', p_pos=0.31773618729615855, p_neg=0.6822638127038408)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.6179351243607946, p_neg=0.3820648756392054)\n",
      "Sentiment(classification='pos', p_pos=0.5511332443109405, p_neg=0.44886675568905915)\n",
      "Sentiment(classification='pos', p_pos=0.9524489694283267, p_neg=0.04755103057167467)\n",
      "Sentiment(classification='pos', p_pos=0.5864864864864867, p_neg=0.4135135135135127)\n",
      "Sentiment(classification='pos', p_pos=0.7907548035777475, p_neg=0.20924519642225212)\n",
      "Sentiment(classification='neg', p_pos=0.39184757997490405, p_neg=0.6081524200250962)\n",
      "Sentiment(classification='neg', p_pos=0.17873278167220275, p_neg=0.8212672183277977)\n",
      "Sentiment(classification='pos', p_pos=0.75, p_neg=0.2499999999999997)\n",
      "Sentiment(classification='pos', p_pos=0.967776073194982, p_neg=0.03222392680501922)\n",
      "Sentiment(classification='neg', p_pos=0.27878754868323985, p_neg=0.7212124513167598)\n",
      "Sentiment(classification='pos', p_pos=0.7975472405917118, p_neg=0.20245275940828736)\n",
      "Sentiment(classification='pos', p_pos=0.997004057773937, p_neg=0.0029959422260625724)\n",
      "Sentiment(classification='pos', p_pos=0.8935056932837301, p_neg=0.10649430671626951)\n",
      "Sentiment(classification='neg', p_pos=0.25452716297786715, p_neg=0.7454728370221334)\n",
      "Sentiment(classification='pos', p_pos=0.5883471651324842, p_neg=0.41165283486751547)\n",
      "Sentiment(classification='pos', p_pos=0.5821654156753677, p_neg=0.4178345843246333)\n",
      "Sentiment(classification='pos', p_pos=0.9391219396583068, p_neg=0.060878060341694844)\n",
      "Sentiment(classification='pos', p_pos=0.9461425912048265, p_neg=0.05385740879517431)\n",
      "Sentiment(classification='pos', p_pos=0.7522282832396124, p_neg=0.24777171676038823)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_text_data(\"datasets/dataset 2021090613_29_33.txt\")\n",
    "# Remove unwanted columns from dataset\n",
    "# n_dataset = remove_unwanted_cols(dataset, ['t_id', 'created_at', 'query', 'user'])\n",
    "#Preprocess data\n",
    "# dataset[\"cleanText\"] = dataset['text']\n",
    "dataset[\"cleanText\"] = dataset['text'].apply(preprocess_tweet_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# determine the sentiment of the tweet using TextBlob and use that result to train the model\n",
    "sent = []\n",
    "for x in dataset['cleanText']:\n",
    "    \n",
    "    \n",
    "    \n",
    "    blob_object = TextBlob(x, analyzer=NaiveBayesAnalyzer())\n",
    "    \n",
    "#    Sentiment(classification='pos', p_pos=0.5057908299783777, p_neg=0.49420917002162196)\n",
    "    \n",
    "    \n",
    "    analysis = blob_object.sentiment\n",
    "    \n",
    "    c = analysis.classification\n",
    "    p = analysis.p_pos\n",
    "    n = analysis.p_neg\n",
    "    \n",
    "    print(analysis)\n",
    "    \n",
    "    if p < 0.4:\n",
    "        r = -1\n",
    "    elif p > 0.6:\n",
    "        r = 1\n",
    "    else:\n",
    "        r = 0\n",
    "    sent.append(r) \n",
    "    \n",
    "#create the sentiment column\n",
    "dataset[\"sentiment\"] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45454545454545453\n",
      "0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into Train, Test\n",
    "\n",
    "# Same tf vector will be used for Testing sentiments on unseen trending data\n",
    "tf_vector = get_feature_vector(np.array(dataset['cleanText']).ravel())\n",
    "X = tf_vector.transform(np.array(dataset['cleanText']).ravel())\n",
    "y = np.array(dataset['sentiment']).ravel()\n",
    "# y = y.astype('float')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "# Training Naive Bayes model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_predict_nb = NB_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_nb))\n",
    "\n",
    "# Training Logistics Regression model\n",
    "LR_model = LogisticRegression(solver='lbfgs')\n",
    "LR_model.fit(X_train, y_train)\n",
    "y_predict_lr = LR_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   User       52 non-null     object\n",
      " 1   Date       52 non-null     object\n",
      " 2   Retweet    52 non-null     int64 \n",
      " 3   text       52 non-null     object\n",
      " 4   cleanText  52 non-null     object\n",
      " 5   sentiment  52 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 2.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emojizedcom</td>\n",
       "      <td>Mon Sep 06 18:29:15 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Monotonectally Mesh Intuitive Manufactured #P...</td>\n",
       "      <td>Monotonectally Mesh Intuitive Manufactured #P...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CurtisSChin</td>\n",
       "      <td>Mon Sep 06 18:28:52 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#USA ðŸ‡ºðŸ‡¸\\n</td>\n",
       "      <td>#USA ðŸ‡ºðŸ‡¸\\n</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RLTraveler</td>\n",
       "      <td>Mon Sep 06 18:16:19 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Masked?  ðŸ¤”ðŸ˜· \\n</td>\n",
       "      <td>Masked?  ðŸ¤”ðŸ˜· \\n</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>clayandbuck</td>\n",
       "      <td>Mon Sep 06 18:14:27 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What does the #Biden administration have to ...</td>\n",
       "      <td>\"What does the #Biden administration have to ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>DickMorrisTweet</td>\n",
       "      <td>Mon Sep 06 18:13:31 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>People Think Better Of Trump As They Get To K...</td>\n",
       "      <td>People Think Better Of Trump As They Get To K...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>wessas68</td>\n",
       "      <td>Mon Sep 06 18:12:40 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@ShaniaTwain Iâ€™ll be there if #biden letâ€™s us...</td>\n",
       "      <td>@ShaniaTwain Iâ€™ll be there if #biden letâ€™s us...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>God4HopeFaith</td>\n",
       "      <td>Mon Sep 06 18:08:50 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@ThatOldPimp @chrislhayes #AfghanistanCrisis ...</td>\n",
       "      <td>@ThatOldPimp @chrislhayes #AfghanistanCrisis ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>leylaboulton</td>\n",
       "      <td>Mon Sep 06 18:04:40 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Letâ€™s hope the #Biden administration can do m...</td>\n",
       "      <td>Letâ€™s hope the #Biden administration can do m...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Zoetnet</td>\n",
       "      <td>Mon Sep 06 18:01:44 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>FLASHBACK: 4 Years Ago #Trump Warned Against ...</td>\n",
       "      <td>FLASHBACK: 4 Years Ago #Trump Warned Against ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>suiwazear</td>\n",
       "      <td>Mon Sep 06 17:59:23 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#Fauci #biden #who #unicef #cdc #aphq #carter...</td>\n",
       "      <td>#Fauci #biden #who #unicef #cdc #aphq #carter...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               User                             Date  Retweet  \\\n",
       "0       emojizedcom   Mon Sep 06 18:29:15 +0000 2021        0   \n",
       "2       CurtisSChin   Mon Sep 06 18:28:52 +0000 2021        0   \n",
       "24       RLTraveler   Mon Sep 06 18:16:19 +0000 2021        0   \n",
       "29      clayandbuck   Mon Sep 06 18:14:27 +0000 2021        0   \n",
       "30  DickMorrisTweet   Mon Sep 06 18:13:31 +0000 2021        0   \n",
       "31         wessas68   Mon Sep 06 18:12:40 +0000 2021        0   \n",
       "38    God4HopeFaith   Mon Sep 06 18:08:50 +0000 2021        0   \n",
       "39     leylaboulton   Mon Sep 06 18:04:40 +0000 2021        0   \n",
       "42          Zoetnet   Mon Sep 06 18:01:44 +0000 2021        0   \n",
       "46        suiwazear   Mon Sep 06 17:59:23 +0000 2021        0   \n",
       "\n",
       "                                                 text  \\\n",
       "0    Monotonectally Mesh Intuitive Manufactured #P...   \n",
       "2                                           #USA ðŸ‡ºðŸ‡¸\\n   \n",
       "24                                     Masked?  ðŸ¤”ðŸ˜· \\n   \n",
       "29   \"What does the #Biden administration have to ...   \n",
       "30   People Think Better Of Trump As They Get To K...   \n",
       "31   @ShaniaTwain Iâ€™ll be there if #biden letâ€™s us...   \n",
       "38   @ThatOldPimp @chrislhayes #AfghanistanCrisis ...   \n",
       "39   Letâ€™s hope the #Biden administration can do m...   \n",
       "42   FLASHBACK: 4 Years Ago #Trump Warned Against ...   \n",
       "46   #Fauci #biden #who #unicef #cdc #aphq #carter...   \n",
       "\n",
       "                                            cleanText  sentiment  \n",
       "0    Monotonectally Mesh Intuitive Manufactured #P...         -1  \n",
       "2                                           #USA ðŸ‡ºðŸ‡¸\\n         -1  \n",
       "24                                     Masked?  ðŸ¤”ðŸ˜· \\n         -1  \n",
       "29   \"What does the #Biden administration have to ...         -1  \n",
       "30   People Think Better Of Trump As They Get To K...         -1  \n",
       "31   @ShaniaTwain Iâ€™ll be there if #biden letâ€™s us...         -1  \n",
       "38   @ThatOldPimp @chrislhayes #AfghanistanCrisis ...         -1  \n",
       "39   Letâ€™s hope the #Biden administration can do m...         -1  \n",
       "42   FLASHBACK: 4 Years Ago #Trump Warned Against ...         -1  \n",
       "46   #Fauci #biden #who #unicef #cdc #aphq #carter...         -1  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['sentiment'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>global_police</td>\n",
       "      <td>Mon Sep 06 18:29:09 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@TheEconomist Good it's such a wonderful time...</td>\n",
       "      <td>@TheEconomist Good it's such a wonderful time...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patriotlady76</td>\n",
       "      <td>Mon Sep 06 18:27:45 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#State department is stopping flights for Ame...</td>\n",
       "      <td>#State department is stopping flights for Ame...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two001snake</td>\n",
       "      <td>Mon Sep 06 18:26:37 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>ALL #Corrupt @USCongress #FakeNews #MSM #TERR...</td>\n",
       "      <td>ALL #Corrupt @USCongress #FakeNews #MSM #TERR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JoanneSpruceC21</td>\n",
       "      <td>Mon Sep 06 18:25:16 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Biden directs federal aid to NY NJ after dea...</td>\n",
       "      <td>\"Biden directs federal aid to NY NJ after dea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shepersists2</td>\n",
       "      <td>Mon Sep 06 18:23:54 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>I am anti war but the way Americans left Afgh...</td>\n",
       "      <td>I am anti war but the way Americans left Afgh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BoesenA</td>\n",
       "      <td>Mon Sep 06 18:22:50 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Americans held hostage at Afghan Airport as R...</td>\n",
       "      <td>Americans held hostage at Afghan Airport as R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JamesLiskutin</td>\n",
       "      <td>Mon Sep 06 18:21:57 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>'Traditional' Muslim rules enforced after Tal...</td>\n",
       "      <td>'Traditional' Muslim rules enforced after Tal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>canine2</td>\n",
       "      <td>Mon Sep 06 18:20:44 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>We didnâ€™t enter #Afghanistan for a 20 year ex...</td>\n",
       "      <td>We didnâ€™t enter #Afghanistan for a 20 year ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SaintlySicilian</td>\n",
       "      <td>Mon Sep 06 18:20:32 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>I know that in certain countries (North Korea...</td>\n",
       "      <td>I know that in certain countries (North Korea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JohnKevinLucke1</td>\n",
       "      <td>Mon Sep 06 18:18:49 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Daily US COVID-19 infections up more than 300...</td>\n",
       "      <td>Daily US COVID-19 infections up more than 300...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Demscorruptlia1</td>\n",
       "      <td>Mon Sep 06 18:16:06 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#biden #milley #dems #liberals #JoeMustGo #Af...</td>\n",
       "      <td>#biden #milley #dems #liberals #JoeMustGo #Af...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DonnyFerguson</td>\n",
       "      <td>Mon Sep 06 18:15:06 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>3 Harmful Consequences of Biden Killing the K...</td>\n",
       "      <td>3 Harmful Consequences of Biden Killing the K...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>EngelhardtDeniz</td>\n",
       "      <td>Mon Sep 06 18:15:03 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ President Joe #Biden approved major disast...</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ President Joe #Biden approved major disast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ChuckNorton1</td>\n",
       "      <td>Mon Sep 06 18:12:24 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Taliban Holding 6 planes full of Americans an...</td>\n",
       "      <td>Taliban Holding 6 planes full of Americans an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>speaker42</td>\n",
       "      <td>Mon Sep 06 18:11:52 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>President Biden is still defending his decisi...</td>\n",
       "      <td>President Biden is still defending his decisi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AjayKauljourno</td>\n",
       "      <td>Mon Sep 06 18:09:09 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Joe #Biden is the modern day Dhritarashtra wh...</td>\n",
       "      <td>Joe #Biden is the modern day Dhritarashtra wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RestDollfaceJMT</td>\n",
       "      <td>Mon Sep 06 18:02:42 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@HenryPersephone @BeachCity55 @JoeBiden @POTU...</td>\n",
       "      <td>@HenryPersephone @BeachCity55 @JoeBiden @POTU...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PurQi</td>\n",
       "      <td>Mon Sep 06 18:02:14 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>After #Treadeau can no longer walk the street...</td>\n",
       "      <td>After #Treadeau can no longer walk the street...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>DonnyFerguson</td>\n",
       "      <td>Mon Sep 06 18:00:26 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Gun Owners Celebrate after Gun Ban Struck Dow...</td>\n",
       "      <td>Gun Owners Celebrate after Gun Ban Struck Dow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>marching201516</td>\n",
       "      <td>Mon Sep 06 18:00:14 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>court case against #EmoryUniversity (#Atlanta...</td>\n",
       "      <td>court case against #EmoryUniversity (#Atlanta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Iustiti52533878</td>\n",
       "      <td>Mon Sep 06 17:59:33 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Democrats are freaking out over Joe Manchin's...</td>\n",
       "      <td>Democrats are freaking out over Joe Manchin's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>EngelhardtDeniz</td>\n",
       "      <td>Mon Sep 06 17:57:52 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ #NYC President Joe #Biden approved major d...</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ #NYC President Joe #Biden approved major d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bluzrocker</td>\n",
       "      <td>Mon Sep 06 17:53:58 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>As long as Americans have the rights to freed...</td>\n",
       "      <td>As long as Americans have the rights to freed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>breaknnews1</td>\n",
       "      <td>Mon Sep 06 17:53:44 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>'Blue' makes green: Orgeron inspires UCLA shi...</td>\n",
       "      <td>'Blue' makes green: Orgeron inspires UCLA shi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               User                             Date  Retweet  \\\n",
       "1     global_police   Mon Sep 06 18:29:09 +0000 2021        0   \n",
       "3     patriotlady76   Mon Sep 06 18:27:45 +0000 2021        0   \n",
       "4       two001snake   Mon Sep 06 18:26:37 +0000 2021        0   \n",
       "5   JoanneSpruceC21   Mon Sep 06 18:25:16 +0000 2021        0   \n",
       "9      shepersists2   Mon Sep 06 18:23:54 +0000 2021        0   \n",
       "11          BoesenA   Mon Sep 06 18:22:50 +0000 2021        0   \n",
       "14    JamesLiskutin   Mon Sep 06 18:21:57 +0000 2021        0   \n",
       "16          canine2   Mon Sep 06 18:20:44 +0000 2021        0   \n",
       "18  SaintlySicilian   Mon Sep 06 18:20:32 +0000 2021        0   \n",
       "20  JohnKevinLucke1   Mon Sep 06 18:18:49 +0000 2021        0   \n",
       "25  Demscorruptlia1   Mon Sep 06 18:16:06 +0000 2021        0   \n",
       "26    DonnyFerguson   Mon Sep 06 18:15:06 +0000 2021        0   \n",
       "27  EngelhardtDeniz   Mon Sep 06 18:15:03 +0000 2021        0   \n",
       "33     ChuckNorton1   Mon Sep 06 18:12:24 +0000 2021        0   \n",
       "35        speaker42   Mon Sep 06 18:11:52 +0000 2021        0   \n",
       "37   AjayKauljourno   Mon Sep 06 18:09:09 +0000 2021        0   \n",
       "40  RestDollfaceJMT   Mon Sep 06 18:02:42 +0000 2021        0   \n",
       "41            PurQi   Mon Sep 06 18:02:14 +0000 2021        0   \n",
       "43    DonnyFerguson   Mon Sep 06 18:00:26 +0000 2021        0   \n",
       "44   marching201516   Mon Sep 06 18:00:14 +0000 2021        0   \n",
       "45  Iustiti52533878   Mon Sep 06 17:59:33 +0000 2021        0   \n",
       "49  EngelhardtDeniz   Mon Sep 06 17:57:52 +0000 2021        0   \n",
       "50       bluzrocker   Mon Sep 06 17:53:58 +0000 2021        0   \n",
       "51      breaknnews1   Mon Sep 06 17:53:44 +0000 2021        0   \n",
       "\n",
       "                                                 text  \\\n",
       "1    @TheEconomist Good it's such a wonderful time...   \n",
       "3    #State department is stopping flights for Ame...   \n",
       "4    ALL #Corrupt @USCongress #FakeNews #MSM #TERR...   \n",
       "5    \"Biden directs federal aid to NY NJ after dea...   \n",
       "9    I am anti war but the way Americans left Afgh...   \n",
       "11   Americans held hostage at Afghan Airport as R...   \n",
       "14   'Traditional' Muslim rules enforced after Tal...   \n",
       "16   We didnâ€™t enter #Afghanistan for a 20 year ex...   \n",
       "18   I know that in certain countries (North Korea...   \n",
       "20   Daily US COVID-19 infections up more than 300...   \n",
       "25   #biden #milley #dems #liberals #JoeMustGo #Af...   \n",
       "26   3 Harmful Consequences of Biden Killing the K...   \n",
       "27   ðŸ‡ºðŸ‡¸ President Joe #Biden approved major disast...   \n",
       "33   Taliban Holding 6 planes full of Americans an...   \n",
       "35   President Biden is still defending his decisi...   \n",
       "37   Joe #Biden is the modern day Dhritarashtra wh...   \n",
       "40   @HenryPersephone @BeachCity55 @JoeBiden @POTU...   \n",
       "41   After #Treadeau can no longer walk the street...   \n",
       "43   Gun Owners Celebrate after Gun Ban Struck Dow...   \n",
       "44   court case against #EmoryUniversity (#Atlanta...   \n",
       "45   Democrats are freaking out over Joe Manchin's...   \n",
       "49   ðŸ‡ºðŸ‡¸ #NYC President Joe #Biden approved major d...   \n",
       "50   As long as Americans have the rights to freed...   \n",
       "51   'Blue' makes green: Orgeron inspires UCLA shi...   \n",
       "\n",
       "                                            cleanText  sentiment  \n",
       "1    @TheEconomist Good it's such a wonderful time...          1  \n",
       "3    #State department is stopping flights for Ame...          1  \n",
       "4    ALL #Corrupt @USCongress #FakeNews #MSM #TERR...          1  \n",
       "5    \"Biden directs federal aid to NY NJ after dea...          1  \n",
       "9    I am anti war but the way Americans left Afgh...          1  \n",
       "11   Americans held hostage at Afghan Airport as R...          1  \n",
       "14   'Traditional' Muslim rules enforced after Tal...          1  \n",
       "16   We didnâ€™t enter #Afghanistan for a 20 year ex...          1  \n",
       "18   I know that in certain countries (North Korea...          1  \n",
       "20   Daily US COVID-19 infections up more than 300...          1  \n",
       "25   #biden #milley #dems #liberals #JoeMustGo #Af...          1  \n",
       "26   3 Harmful Consequences of Biden Killing the K...          1  \n",
       "27   ðŸ‡ºðŸ‡¸ President Joe #Biden approved major disast...          1  \n",
       "33   Taliban Holding 6 planes full of Americans an...          1  \n",
       "35   President Biden is still defending his decisi...          1  \n",
       "37   Joe #Biden is the modern day Dhritarashtra wh...          1  \n",
       "40   @HenryPersephone @BeachCity55 @JoeBiden @POTU...          1  \n",
       "41   After #Treadeau can no longer walk the street...          1  \n",
       "43   Gun Owners Celebrate after Gun Ban Struck Dow...          1  \n",
       "44   court case against #EmoryUniversity (#Atlanta...          1  \n",
       "45   Democrats are freaking out over Joe Manchin's...          1  \n",
       "49   ðŸ‡ºðŸ‡¸ #NYC President Joe #Biden approved major d...          1  \n",
       "50   As long as Americans have the rights to freed...          1  \n",
       "51   'Blue' makes green: Orgeron inspires UCLA shi...          1  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['sentiment'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.375     , 0.54305869, 0.27878755, 0.8832313 , 0.51688715,\n",
       "       0.84646178, 0.58648649, 0.75393727, 0.31773619, 0.5       ,\n",
       "       0.97727518, 0.99700406, 0.5       , 0.75      , 0.94614259,\n",
       "       0.5       , 0.69749636, 0.25452716, 0.5       , 0.73666328,\n",
       "       0.11503524, 0.06184899, 0.828164  , 0.46076328, 0.750341  ,\n",
       "       0.58834717, 0.39184758, 0.98545998, 0.95244897, 0.83241243,\n",
       "       0.53877836, 0.96777607, 0.6571729 , 0.5       , 0.39473684,\n",
       "       0.50433723, 0.5       , 0.93912194, 0.89350569, 0.75222828,\n",
       "       0.7907548 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine  \n",
    "from sqlalchemy import Table, Column, String, MetaData, Integer\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "Base = declarative_base()\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "\n",
    "db_string = 'postgresql://postgres:drwho@localhost/twitter'\n",
    "engine = create_engine(db_string)\n",
    "Base.metadata.create_all(engine)\n",
    "session = Session(bind=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetData (Base):\n",
    "    __tablename__ = \"tweet_data\"\n",
    "    id = Column(Integer, primary_key = True)\n",
    "    name = Column(String)\n",
    "    date = Column(String)\n",
    "    retweet_count = Column(Integer)\n",
    "    tweet_text = Column(String)\n",
    "    tweet_cleaned = Column(String)\n",
    "    favorite_count = Column(Integer)\n",
    "    est_positivity = Column(Integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeModel = False\n",
    "tweets = session.query(TweetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Just got #engaged to the brightest #star in this dark sky. Sheâ€™s my #lighthouse in the #storm. \n",
      "\n",
      "['got', 'engaged', 'bright', 'star', 'dark', 'sky', 'â€™', 'lighthouse', 'storm']\n",
      "  (0, 3)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 6)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (7, 4)\t1.0\n",
      "  (8, 7)\t1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-0180e83dbdb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNB_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;34m\"\"\"Calculate the posterior log probability of the samples X\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    778\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet in tweets:\n",
    "    print(tweet.tweet_text)\n",
    "    cleanTweet = preprocess_tweet_text(tweet.tweet_text)\n",
    "    \n",
    "    v = cleanTweet.split(\" \")\n",
    "    print(v)\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(v)\n",
    "    \n",
    "    X = vector.transform(v)\n",
    "    \n",
    "    print(X)\n",
    "\n",
    "\n",
    "    \n",
    "    if makeModel:\n",
    "        blob_object = TextBlob(cleanTweet, analyzer=NaiveBayesAnalyzer())\n",
    "        analysis = blob_object.sentiment\n",
    "\n",
    "        c = analysis.classification\n",
    "        p = analysis.p_pos\n",
    "        n = analysis.p_neg\n",
    "\n",
    "        if p < 0.4:\n",
    "            r = -1\n",
    "        elif p > 0.6:\n",
    "            r = 1\n",
    "        else:\n",
    "            r = 0\n",
    "    else:\n",
    "        r = NB_model.predict(X)\n",
    "        \n",
    "        \n",
    "    tweet.est_positivity = r \n",
    "    session.update(TweetData).values(est_positivity=r)\n",
    "    session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "c1e83760cadaf87b0a0f9ca95f844ef4947bb6840a3db1438c78c07aa1fb0afe"
  },
  "kernelspec": {
   "display_name": "Python Data",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
