{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/theDoctor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='latin-1')\n",
    "    df.columns = [\"text\",\"sentiment\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_text_data(filename, append = False):\n",
    "    if not append:\n",
    "        data = []\n",
    "        \n",
    "    textfile = open(filename, \"r\")\n",
    "    for tweet in textfile:\n",
    "        parts = tweet.split(',')\n",
    "        if len(parts) > 2:\n",
    "            newTweet = [parts[0],parts[1], 0, \"\".join(parts[3:])]\n",
    "            data.append(newTweet)\n",
    "    textfile.close()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = [\"User\", \"Date\", \"Retweet\", \"text\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def delete_redundant_cols(df, cols):\n",
    "    for col in cols:\n",
    "        del df[col]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi preparing exams'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\",\"\",tweet, flags=re.MULTILINE)   \n",
    "\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "    tweet = re.sub(r'\\@\\w+|\\#',\"\",tweet)\n",
    "\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stop_words]\n",
    "\n",
    "#     ps = PorterStemmer()\n",
    "#     stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]\n",
    "\n",
    "    return \" \".join(lemma_words)\n",
    "preprocess_tweet_text(\"Hi there, how are you preparing for your exams?\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_string(sentiment):\n",
    "    if sentiment < -0.1:\n",
    "        return \"Negative\"\n",
    "    elif sentiment < 0.1:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(classification='neg', p_pos=0.061848992071306515, p_neg=0.9381510079286948)\n",
      "Sentiment(classification='pos', p_pos=0.8324124318829695, p_neg=0.1675875681170304)\n",
      "Sentiment(classification='neg', p_pos=0.3947368421052628, p_neg=0.6052631578947371)\n",
      "Sentiment(classification='pos', p_pos=0.6571729015199701, p_neg=0.34282709848003085)\n",
      "Sentiment(classification='pos', p_pos=0.697496358192723, p_neg=0.30250364180727796)\n",
      "Sentiment(classification='pos', p_pos=0.84646178324069, p_neg=0.1535382167593091)\n",
      "Sentiment(classification='neg', p_pos=0.46076328060400257, p_neg=0.5392367193959956)\n",
      "Sentiment(classification='pos', p_pos=0.5387783635019395, p_neg=0.4612216364980616)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.8832313000416809, p_neg=0.11676869995831875)\n",
      "Sentiment(classification='pos', p_pos=0.5953133228014746, p_neg=0.4046866771985255)\n",
      "Sentiment(classification='pos', p_pos=0.7366632825412875, p_neg=0.26333671745871584)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.9772751778514385, p_neg=0.02272482214855901)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.7503410007886737, p_neg=0.24965899921132556)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.9854599805988188, p_neg=0.014540019401178854)\n",
      "Sentiment(classification='pos', p_pos=0.5168871501131187, p_neg=0.48311284988688197)\n",
      "Sentiment(classification='pos', p_pos=0.8478153195563332, p_neg=0.1521846804436644)\n",
      "Sentiment(classification='pos', p_pos=0.5430586948045787, p_neg=0.4569413051954218)\n",
      "Sentiment(classification='neg', p_pos=0.4285243383415213, p_neg=0.5714756616584777)\n",
      "Sentiment(classification='pos', p_pos=0.5043372294908613, p_neg=0.49566277050913865)\n",
      "Sentiment(classification='neg', p_pos=0.37500000000000044, p_neg=0.6249999999999998)\n",
      "Sentiment(classification='pos', p_pos=0.75, p_neg=0.2499999999999997)\n",
      "Sentiment(classification='pos', p_pos=0.7539372683057236, p_neg=0.24606273169427836)\n",
      "Sentiment(classification='pos', p_pos=0.8281640040026621, p_neg=0.1718359959973357)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='neg', p_pos=0.1803212361203609, p_neg=0.8196787638796381)\n",
      "Sentiment(classification='neg', p_pos=0.11503524477202483, p_neg=0.8849647552279747)\n",
      "Sentiment(classification='neg', p_pos=0.31773618729615855, p_neg=0.6822638127038408)\n",
      "Sentiment(classification='pos', p_pos=0.5, p_neg=0.5)\n",
      "Sentiment(classification='pos', p_pos=0.6179351243607946, p_neg=0.3820648756392054)\n",
      "Sentiment(classification='pos', p_pos=0.5511332443109405, p_neg=0.44886675568905915)\n",
      "Sentiment(classification='pos', p_pos=0.9524489694283267, p_neg=0.04755103057167467)\n",
      "Sentiment(classification='pos', p_pos=0.5864864864864867, p_neg=0.4135135135135127)\n",
      "Sentiment(classification='pos', p_pos=0.7907548035777475, p_neg=0.20924519642225212)\n",
      "Sentiment(classification='neg', p_pos=0.39184757997490405, p_neg=0.6081524200250962)\n",
      "Sentiment(classification='neg', p_pos=0.17873278167220275, p_neg=0.8212672183277977)\n",
      "Sentiment(classification='pos', p_pos=0.75, p_neg=0.2499999999999997)\n",
      "Sentiment(classification='pos', p_pos=0.967776073194982, p_neg=0.03222392680501922)\n",
      "Sentiment(classification='neg', p_pos=0.27878754868323985, p_neg=0.7212124513167598)\n",
      "Sentiment(classification='pos', p_pos=0.7975472405917118, p_neg=0.20245275940828736)\n",
      "Sentiment(classification='pos', p_pos=0.997004057773937, p_neg=0.0029959422260625724)\n",
      "Sentiment(classification='pos', p_pos=0.8935056932837301, p_neg=0.10649430671626951)\n",
      "Sentiment(classification='neg', p_pos=0.25452716297786715, p_neg=0.7454728370221334)\n",
      "Sentiment(classification='pos', p_pos=0.5883471651324842, p_neg=0.41165283486751547)\n",
      "Sentiment(classification='pos', p_pos=0.5821654156753677, p_neg=0.4178345843246333)\n",
      "Sentiment(classification='pos', p_pos=0.9391219396583068, p_neg=0.060878060341694844)\n",
      "Sentiment(classification='pos', p_pos=0.9461425912048265, p_neg=0.05385740879517431)\n",
      "Sentiment(classification='pos', p_pos=0.7522282832396124, p_neg=0.24777171676038823)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_text_data(\"datasets/dataset 2021090613_29_33.txt\")\n",
    "# Remove unwanted columns from dataset\n",
    "# n_dataset = remove_unwanted_cols(dataset, ['t_id', 'created_at', 'query', 'user'])\n",
    "#Preprocess data\n",
    "# dataset[\"cleanText\"] = dataset['text']\n",
    "dataset[\"cleanText\"] = dataset['text'].apply(preprocess_tweet_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# determine the sentiment of the tweet using TextBlob and use that result to train the model\n",
    "sent = []\n",
    "for x in dataset['cleanText']:\n",
    "    \n",
    "    \n",
    "    \n",
    "    blob_object = TextBlob(x, analyzer=NaiveBayesAnalyzer())\n",
    "    \n",
    "#    Sentiment(classification='pos', p_pos=0.5057908299783777, p_neg=0.49420917002162196)\n",
    "    \n",
    "    \n",
    "    analysis = blob_object.sentiment\n",
    "    \n",
    "    c = analysis.classification\n",
    "    p = analysis.p_pos\n",
    "    n = analysis.p_neg\n",
    "    \n",
    "    print(analysis)\n",
    "    \n",
    "    if p < 0.4:\n",
    "        r = -1\n",
    "    elif p > 0.6:\n",
    "        r = 1\n",
    "    else:\n",
    "        r = 0\n",
    "    sent.append(r) \n",
    "    \n",
    "#create the sentiment column\n",
    "dataset[\"sentiment\"] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45454545454545453\n",
      "0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into Train, Test\n",
    "\n",
    "# Same tf vector will be used for Testing sentiments on unseen trending data\n",
    "tf_vector = get_feature_vector(np.array(dataset['cleanText']).ravel())\n",
    "X = tf_vector.transform(np.array(dataset['cleanText']).ravel())\n",
    "y = np.array(dataset['sentiment']).ravel()\n",
    "# y = y.astype('float')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "# Training Naive Bayes model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_predict_nb = NB_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_nb))\n",
    "\n",
    "# Training Logistics Regression model\n",
    "LR_model = LogisticRegression(solver='lbfgs')\n",
    "LR_model.fit(X_train, y_train)\n",
    "y_predict_lr = LR_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   User       52 non-null     object\n",
      " 1   Date       52 non-null     object\n",
      " 2   Retweet    52 non-null     int64 \n",
      " 3   text       52 non-null     object\n",
      " 4   cleanText  52 non-null     object\n",
      " 5   sentiment  52 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 2.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emojizedcom</td>\n",
       "      <td>Mon Sep 06 18:29:15 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Monotonectally Mesh Intuitive Manufactured #P...</td>\n",
       "      <td>Monotonectally Mesh Intuitive Manufactured #P...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CurtisSChin</td>\n",
       "      <td>Mon Sep 06 18:28:52 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#USA 🇺🇸\\n</td>\n",
       "      <td>#USA 🇺🇸\\n</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RLTraveler</td>\n",
       "      <td>Mon Sep 06 18:16:19 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Masked?  🤔😷 \\n</td>\n",
       "      <td>Masked?  🤔😷 \\n</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>clayandbuck</td>\n",
       "      <td>Mon Sep 06 18:14:27 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What does the #Biden administration have to ...</td>\n",
       "      <td>\"What does the #Biden administration have to ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>DickMorrisTweet</td>\n",
       "      <td>Mon Sep 06 18:13:31 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>People Think Better Of Trump As They Get To K...</td>\n",
       "      <td>People Think Better Of Trump As They Get To K...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>wessas68</td>\n",
       "      <td>Mon Sep 06 18:12:40 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@ShaniaTwain I’ll be there if #biden let’s us...</td>\n",
       "      <td>@ShaniaTwain I’ll be there if #biden let’s us...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>God4HopeFaith</td>\n",
       "      <td>Mon Sep 06 18:08:50 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@ThatOldPimp @chrislhayes #AfghanistanCrisis ...</td>\n",
       "      <td>@ThatOldPimp @chrislhayes #AfghanistanCrisis ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>leylaboulton</td>\n",
       "      <td>Mon Sep 06 18:04:40 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Let’s hope the #Biden administration can do m...</td>\n",
       "      <td>Let’s hope the #Biden administration can do m...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Zoetnet</td>\n",
       "      <td>Mon Sep 06 18:01:44 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>FLASHBACK: 4 Years Ago #Trump Warned Against ...</td>\n",
       "      <td>FLASHBACK: 4 Years Ago #Trump Warned Against ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>suiwazear</td>\n",
       "      <td>Mon Sep 06 17:59:23 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#Fauci #biden #who #unicef #cdc #aphq #carter...</td>\n",
       "      <td>#Fauci #biden #who #unicef #cdc #aphq #carter...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               User                             Date  Retweet  \\\n",
       "0       emojizedcom   Mon Sep 06 18:29:15 +0000 2021        0   \n",
       "2       CurtisSChin   Mon Sep 06 18:28:52 +0000 2021        0   \n",
       "24       RLTraveler   Mon Sep 06 18:16:19 +0000 2021        0   \n",
       "29      clayandbuck   Mon Sep 06 18:14:27 +0000 2021        0   \n",
       "30  DickMorrisTweet   Mon Sep 06 18:13:31 +0000 2021        0   \n",
       "31         wessas68   Mon Sep 06 18:12:40 +0000 2021        0   \n",
       "38    God4HopeFaith   Mon Sep 06 18:08:50 +0000 2021        0   \n",
       "39     leylaboulton   Mon Sep 06 18:04:40 +0000 2021        0   \n",
       "42          Zoetnet   Mon Sep 06 18:01:44 +0000 2021        0   \n",
       "46        suiwazear   Mon Sep 06 17:59:23 +0000 2021        0   \n",
       "\n",
       "                                                 text  \\\n",
       "0    Monotonectally Mesh Intuitive Manufactured #P...   \n",
       "2                                           #USA 🇺🇸\\n   \n",
       "24                                     Masked?  🤔😷 \\n   \n",
       "29   \"What does the #Biden administration have to ...   \n",
       "30   People Think Better Of Trump As They Get To K...   \n",
       "31   @ShaniaTwain I’ll be there if #biden let’s us...   \n",
       "38   @ThatOldPimp @chrislhayes #AfghanistanCrisis ...   \n",
       "39   Let’s hope the #Biden administration can do m...   \n",
       "42   FLASHBACK: 4 Years Ago #Trump Warned Against ...   \n",
       "46   #Fauci #biden #who #unicef #cdc #aphq #carter...   \n",
       "\n",
       "                                            cleanText  sentiment  \n",
       "0    Monotonectally Mesh Intuitive Manufactured #P...         -1  \n",
       "2                                           #USA 🇺🇸\\n         -1  \n",
       "24                                     Masked?  🤔😷 \\n         -1  \n",
       "29   \"What does the #Biden administration have to ...         -1  \n",
       "30   People Think Better Of Trump As They Get To K...         -1  \n",
       "31   @ShaniaTwain I’ll be there if #biden let’s us...         -1  \n",
       "38   @ThatOldPimp @chrislhayes #AfghanistanCrisis ...         -1  \n",
       "39   Let’s hope the #Biden administration can do m...         -1  \n",
       "42   FLASHBACK: 4 Years Ago #Trump Warned Against ...         -1  \n",
       "46   #Fauci #biden #who #unicef #cdc #aphq #carter...         -1  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['sentiment'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>global_police</td>\n",
       "      <td>Mon Sep 06 18:29:09 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@TheEconomist Good it's such a wonderful time...</td>\n",
       "      <td>@TheEconomist Good it's such a wonderful time...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patriotlady76</td>\n",
       "      <td>Mon Sep 06 18:27:45 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#State department is stopping flights for Ame...</td>\n",
       "      <td>#State department is stopping flights for Ame...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two001snake</td>\n",
       "      <td>Mon Sep 06 18:26:37 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>ALL #Corrupt @USCongress #FakeNews #MSM #TERR...</td>\n",
       "      <td>ALL #Corrupt @USCongress #FakeNews #MSM #TERR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JoanneSpruceC21</td>\n",
       "      <td>Mon Sep 06 18:25:16 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Biden directs federal aid to NY NJ after dea...</td>\n",
       "      <td>\"Biden directs federal aid to NY NJ after dea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shepersists2</td>\n",
       "      <td>Mon Sep 06 18:23:54 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>I am anti war but the way Americans left Afgh...</td>\n",
       "      <td>I am anti war but the way Americans left Afgh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BoesenA</td>\n",
       "      <td>Mon Sep 06 18:22:50 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Americans held hostage at Afghan Airport as R...</td>\n",
       "      <td>Americans held hostage at Afghan Airport as R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JamesLiskutin</td>\n",
       "      <td>Mon Sep 06 18:21:57 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>'Traditional' Muslim rules enforced after Tal...</td>\n",
       "      <td>'Traditional' Muslim rules enforced after Tal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>canine2</td>\n",
       "      <td>Mon Sep 06 18:20:44 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>We didn’t enter #Afghanistan for a 20 year ex...</td>\n",
       "      <td>We didn’t enter #Afghanistan for a 20 year ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SaintlySicilian</td>\n",
       "      <td>Mon Sep 06 18:20:32 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>I know that in certain countries (North Korea...</td>\n",
       "      <td>I know that in certain countries (North Korea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JohnKevinLucke1</td>\n",
       "      <td>Mon Sep 06 18:18:49 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Daily US COVID-19 infections up more than 300...</td>\n",
       "      <td>Daily US COVID-19 infections up more than 300...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Demscorruptlia1</td>\n",
       "      <td>Mon Sep 06 18:16:06 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>#biden #milley #dems #liberals #JoeMustGo #Af...</td>\n",
       "      <td>#biden #milley #dems #liberals #JoeMustGo #Af...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DonnyFerguson</td>\n",
       "      <td>Mon Sep 06 18:15:06 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>3 Harmful Consequences of Biden Killing the K...</td>\n",
       "      <td>3 Harmful Consequences of Biden Killing the K...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>EngelhardtDeniz</td>\n",
       "      <td>Mon Sep 06 18:15:03 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>🇺🇸 President Joe #Biden approved major disast...</td>\n",
       "      <td>🇺🇸 President Joe #Biden approved major disast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ChuckNorton1</td>\n",
       "      <td>Mon Sep 06 18:12:24 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Taliban Holding 6 planes full of Americans an...</td>\n",
       "      <td>Taliban Holding 6 planes full of Americans an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>speaker42</td>\n",
       "      <td>Mon Sep 06 18:11:52 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>President Biden is still defending his decisi...</td>\n",
       "      <td>President Biden is still defending his decisi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AjayKauljourno</td>\n",
       "      <td>Mon Sep 06 18:09:09 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Joe #Biden is the modern day Dhritarashtra wh...</td>\n",
       "      <td>Joe #Biden is the modern day Dhritarashtra wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RestDollfaceJMT</td>\n",
       "      <td>Mon Sep 06 18:02:42 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>@HenryPersephone @BeachCity55 @JoeBiden @POTU...</td>\n",
       "      <td>@HenryPersephone @BeachCity55 @JoeBiden @POTU...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PurQi</td>\n",
       "      <td>Mon Sep 06 18:02:14 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>After #Treadeau can no longer walk the street...</td>\n",
       "      <td>After #Treadeau can no longer walk the street...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>DonnyFerguson</td>\n",
       "      <td>Mon Sep 06 18:00:26 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Gun Owners Celebrate after Gun Ban Struck Dow...</td>\n",
       "      <td>Gun Owners Celebrate after Gun Ban Struck Dow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>marching201516</td>\n",
       "      <td>Mon Sep 06 18:00:14 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>court case against #EmoryUniversity (#Atlanta...</td>\n",
       "      <td>court case against #EmoryUniversity (#Atlanta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Iustiti52533878</td>\n",
       "      <td>Mon Sep 06 17:59:33 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Democrats are freaking out over Joe Manchin's...</td>\n",
       "      <td>Democrats are freaking out over Joe Manchin's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>EngelhardtDeniz</td>\n",
       "      <td>Mon Sep 06 17:57:52 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>🇺🇸 #NYC President Joe #Biden approved major d...</td>\n",
       "      <td>🇺🇸 #NYC President Joe #Biden approved major d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>bluzrocker</td>\n",
       "      <td>Mon Sep 06 17:53:58 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>As long as Americans have the rights to freed...</td>\n",
       "      <td>As long as Americans have the rights to freed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>breaknnews1</td>\n",
       "      <td>Mon Sep 06 17:53:44 +0000 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>'Blue' makes green: Orgeron inspires UCLA shi...</td>\n",
       "      <td>'Blue' makes green: Orgeron inspires UCLA shi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               User                             Date  Retweet  \\\n",
       "1     global_police   Mon Sep 06 18:29:09 +0000 2021        0   \n",
       "3     patriotlady76   Mon Sep 06 18:27:45 +0000 2021        0   \n",
       "4       two001snake   Mon Sep 06 18:26:37 +0000 2021        0   \n",
       "5   JoanneSpruceC21   Mon Sep 06 18:25:16 +0000 2021        0   \n",
       "9      shepersists2   Mon Sep 06 18:23:54 +0000 2021        0   \n",
       "11          BoesenA   Mon Sep 06 18:22:50 +0000 2021        0   \n",
       "14    JamesLiskutin   Mon Sep 06 18:21:57 +0000 2021        0   \n",
       "16          canine2   Mon Sep 06 18:20:44 +0000 2021        0   \n",
       "18  SaintlySicilian   Mon Sep 06 18:20:32 +0000 2021        0   \n",
       "20  JohnKevinLucke1   Mon Sep 06 18:18:49 +0000 2021        0   \n",
       "25  Demscorruptlia1   Mon Sep 06 18:16:06 +0000 2021        0   \n",
       "26    DonnyFerguson   Mon Sep 06 18:15:06 +0000 2021        0   \n",
       "27  EngelhardtDeniz   Mon Sep 06 18:15:03 +0000 2021        0   \n",
       "33     ChuckNorton1   Mon Sep 06 18:12:24 +0000 2021        0   \n",
       "35        speaker42   Mon Sep 06 18:11:52 +0000 2021        0   \n",
       "37   AjayKauljourno   Mon Sep 06 18:09:09 +0000 2021        0   \n",
       "40  RestDollfaceJMT   Mon Sep 06 18:02:42 +0000 2021        0   \n",
       "41            PurQi   Mon Sep 06 18:02:14 +0000 2021        0   \n",
       "43    DonnyFerguson   Mon Sep 06 18:00:26 +0000 2021        0   \n",
       "44   marching201516   Mon Sep 06 18:00:14 +0000 2021        0   \n",
       "45  Iustiti52533878   Mon Sep 06 17:59:33 +0000 2021        0   \n",
       "49  EngelhardtDeniz   Mon Sep 06 17:57:52 +0000 2021        0   \n",
       "50       bluzrocker   Mon Sep 06 17:53:58 +0000 2021        0   \n",
       "51      breaknnews1   Mon Sep 06 17:53:44 +0000 2021        0   \n",
       "\n",
       "                                                 text  \\\n",
       "1    @TheEconomist Good it's such a wonderful time...   \n",
       "3    #State department is stopping flights for Ame...   \n",
       "4    ALL #Corrupt @USCongress #FakeNews #MSM #TERR...   \n",
       "5    \"Biden directs federal aid to NY NJ after dea...   \n",
       "9    I am anti war but the way Americans left Afgh...   \n",
       "11   Americans held hostage at Afghan Airport as R...   \n",
       "14   'Traditional' Muslim rules enforced after Tal...   \n",
       "16   We didn’t enter #Afghanistan for a 20 year ex...   \n",
       "18   I know that in certain countries (North Korea...   \n",
       "20   Daily US COVID-19 infections up more than 300...   \n",
       "25   #biden #milley #dems #liberals #JoeMustGo #Af...   \n",
       "26   3 Harmful Consequences of Biden Killing the K...   \n",
       "27   🇺🇸 President Joe #Biden approved major disast...   \n",
       "33   Taliban Holding 6 planes full of Americans an...   \n",
       "35   President Biden is still defending his decisi...   \n",
       "37   Joe #Biden is the modern day Dhritarashtra wh...   \n",
       "40   @HenryPersephone @BeachCity55 @JoeBiden @POTU...   \n",
       "41   After #Treadeau can no longer walk the street...   \n",
       "43   Gun Owners Celebrate after Gun Ban Struck Dow...   \n",
       "44   court case against #EmoryUniversity (#Atlanta...   \n",
       "45   Democrats are freaking out over Joe Manchin's...   \n",
       "49   🇺🇸 #NYC President Joe #Biden approved major d...   \n",
       "50   As long as Americans have the rights to freed...   \n",
       "51   'Blue' makes green: Orgeron inspires UCLA shi...   \n",
       "\n",
       "                                            cleanText  sentiment  \n",
       "1    @TheEconomist Good it's such a wonderful time...          1  \n",
       "3    #State department is stopping flights for Ame...          1  \n",
       "4    ALL #Corrupt @USCongress #FakeNews #MSM #TERR...          1  \n",
       "5    \"Biden directs federal aid to NY NJ after dea...          1  \n",
       "9    I am anti war but the way Americans left Afgh...          1  \n",
       "11   Americans held hostage at Afghan Airport as R...          1  \n",
       "14   'Traditional' Muslim rules enforced after Tal...          1  \n",
       "16   We didn’t enter #Afghanistan for a 20 year ex...          1  \n",
       "18   I know that in certain countries (North Korea...          1  \n",
       "20   Daily US COVID-19 infections up more than 300...          1  \n",
       "25   #biden #milley #dems #liberals #JoeMustGo #Af...          1  \n",
       "26   3 Harmful Consequences of Biden Killing the K...          1  \n",
       "27   🇺🇸 President Joe #Biden approved major disast...          1  \n",
       "33   Taliban Holding 6 planes full of Americans an...          1  \n",
       "35   President Biden is still defending his decisi...          1  \n",
       "37   Joe #Biden is the modern day Dhritarashtra wh...          1  \n",
       "40   @HenryPersephone @BeachCity55 @JoeBiden @POTU...          1  \n",
       "41   After #Treadeau can no longer walk the street...          1  \n",
       "43   Gun Owners Celebrate after Gun Ban Struck Dow...          1  \n",
       "44   court case against #EmoryUniversity (#Atlanta...          1  \n",
       "45   Democrats are freaking out over Joe Manchin's...          1  \n",
       "49   🇺🇸 #NYC President Joe #Biden approved major d...          1  \n",
       "50   As long as Americans have the rights to freed...          1  \n",
       "51   'Blue' makes green: Orgeron inspires UCLA shi...          1  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['sentiment'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.375     , 0.54305869, 0.27878755, 0.8832313 , 0.51688715,\n",
       "       0.84646178, 0.58648649, 0.75393727, 0.31773619, 0.5       ,\n",
       "       0.97727518, 0.99700406, 0.5       , 0.75      , 0.94614259,\n",
       "       0.5       , 0.69749636, 0.25452716, 0.5       , 0.73666328,\n",
       "       0.11503524, 0.06184899, 0.828164  , 0.46076328, 0.750341  ,\n",
       "       0.58834717, 0.39184758, 0.98545998, 0.95244897, 0.83241243,\n",
       "       0.53877836, 0.96777607, 0.6571729 , 0.5       , 0.39473684,\n",
       "       0.50433723, 0.5       , 0.93912194, 0.89350569, 0.75222828,\n",
       "       0.7907548 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine  \n",
    "from sqlalchemy import Table, Column, String, MetaData, Integer\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "Base = declarative_base()\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "\n",
    "db_string = 'postgresql://postgres:drwho@localhost/twitter'\n",
    "engine = create_engine(db_string)\n",
    "Base.metadata.create_all(engine)\n",
    "session = Session(bind=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetData (Base):\n",
    "    __tablename__ = \"tweet_data\"\n",
    "    id = Column(Integer, primary_key = True)\n",
    "    name = Column(String)\n",
    "    date = Column(String)\n",
    "    retweet_count = Column(Integer)\n",
    "    tweet_text = Column(String)\n",
    "    tweet_cleaned = Column(String)\n",
    "    favorite_count = Column(Integer)\n",
    "    est_positivity = Column(Integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeModel = False\n",
    "tweets = session.query(TweetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Just got #engaged to the brightest #star in this dark sky. She’s my #lighthouse in the #storm. \n",
      "\n",
      "['got', 'engaged', 'bright', 'star', 'dark', 'sky', '’', 'lighthouse', 'storm']\n",
      "  (0, 3)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 6)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (7, 4)\t1.0\n",
      "  (8, 7)\t1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-0180e83dbdb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNB_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;34m\"\"\"Calculate the posterior log probability of the samples X\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    778\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet in tweets:\n",
    "    print(tweet.tweet_text)\n",
    "    cleanTweet = preprocess_tweet_text(tweet.tweet_text)\n",
    "    \n",
    "    v = cleanTweet.split(\" \")\n",
    "    print(v)\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(v)\n",
    "    \n",
    "    X = vector.transform(v)\n",
    "    \n",
    "    print(X)\n",
    "\n",
    "\n",
    "    \n",
    "    if makeModel:\n",
    "        blob_object = TextBlob(cleanTweet, analyzer=NaiveBayesAnalyzer())\n",
    "        analysis = blob_object.sentiment\n",
    "\n",
    "        c = analysis.classification\n",
    "        p = analysis.p_pos\n",
    "        n = analysis.p_neg\n",
    "\n",
    "        if p < 0.4:\n",
    "            r = -1\n",
    "        elif p > 0.6:\n",
    "            r = 1\n",
    "        else:\n",
    "            r = 0\n",
    "    else:\n",
    "        r = NB_model.predict(X)\n",
    "        \n",
    "        \n",
    "    tweet.est_positivity = r \n",
    "    session.update(TweetData).values(est_positivity=r)\n",
    "    session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "c1e83760cadaf87b0a0f9ca95f844ef4947bb6840a3db1438c78c07aa1fb0afe"
  },
  "kernelspec": {
   "display_name": "Python Data",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
